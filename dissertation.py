# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zQUis4LmLtvjMifeWLKDy2I0nJnCggdE
"""

import pandas as pd
from google.colab import data_table

# Enable the interactive data table display
data_table.enable_dataframe_formatter()

# Load dataset
file_path = 'diabetesnew.csv'
data = pd.read_csv(file_path)

# Display the dataset
data

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load the dataset
file_path = 'diabetesnew.csv'
data = pd.read_csv(file_path)

# Remove the 'DoctorInCharge' column
data_cleaned = data.drop(columns=['DoctorInCharge'])

# Replace biologically implausible zeros with NaN and handle missing values
columns_with_zeros = ['FastingBloodSugar', 'SystolicBP', 'DiastolicBP', 'BMI']
data_cleaned[columns_with_zeros] = data_cleaned[columns_with_zeros].replace(0, pd.NA)
data_cleaned.fillna(data_cleaned.median(), inplace=True)

# Encode categorical variables
categorical_columns = ['Gender', 'Ethnicity']
data_encoded = pd.get_dummies(data_cleaned, columns=categorical_columns, drop_first=True)

# Define features (X) and target variable (y)
X = data_encoded.drop(columns=['Diagnosis', 'PatientID'])
y = data_encoded['Diagnosis'].astype('int')

data

"""The dataset contains 1879 rows and 46 attributes.

EDA
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import math


file_path = 'diabetesnew.csv'
data = pd.read_csv(file_path)

# Data Overview
print("First few rows of the dataset:")
print(data.head())

print("\nSummary statistics of the dataset:")
print(data.describe())

# Missing Values Analysis
print("\nMissing values in the dataset:")
print(data.isnull().sum())

# Remove the 'DoctorInCharge' column as it contains non-numeric values
data_cleaned = data.drop(columns=['DoctorInCharge'])

# Distribution Analysis
numerical_features = data_cleaned.select_dtypes(include=['float64', 'int64']).columns

# Histograms
num_features = len(numerical_features)
num_cols = 4
num_rows = math.ceil(num_features / num_cols)

plt.figure(figsize=(20, 5 * num_rows))
for i, feature in enumerate(numerical_features):
    plt.subplot(num_rows, num_cols, i + 1)
    sns.histplot(data_cleaned[feature], kde=True)
    plt.title(f'Distribution of {feature}')
plt.tight_layout()
plt.show()

# Box Plots
plt.figure(figsize=(20, 5 * num_rows))
for i, feature in enumerate(numerical_features):
    plt.subplot(num_rows, num_cols, i + 1)
    sns.boxplot(data_cleaned[feature])
    plt.title(f'Box plot of {feature}')
plt.tight_layout()
plt.show()

# Correlation Analysis
plt.figure(figsize=(15, 10))
correlation_matrix = data_cleaned.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()



"""Here ,we see there is no missing value,The dataset provides summary statistics for 1,879 patients, covering various demographic, health, and lifestyle attributes.here we find out Histogram ,Box plot and Correlation Matrix.

Data preprosseing
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler



# Remove the 'DoctorInCharge' column
data_cleaned = data.drop(columns=['DoctorInCharge'])

# Encode categorical variables (like Gender, Ethnicity) into dummy/indicator variables
categorical_columns = ['Gender', 'Ethnicity']
data_encoded = pd.get_dummies(data_cleaned, columns=categorical_columns, drop_first=True)

# Define the features (X) and the target variable (y)
X = data_encoded.drop(columns=['Diagnosis', 'PatientID'])
y = data_encoded['Diagnosis'].astype('int')

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the numerical features (scale them to have mean 0 and variance 1)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""Model training

logistic regresion
"""

import pandas as pd
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt


# Fit the Logistic Regression model
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)

# Retrieve the coefficients
coefficients = log_reg.coef_[0]

# Map the coefficients to their corresponding feature names
feature_coefficients = pd.Series(coefficients, index=X.columns)

# Display the coefficients
print("Logistic Regression Coefficients:")
print(feature_coefficients)

# Identify the most significant variable
most_significant_variable = feature_coefficients.abs().idxmax()
print(f"\nThe most significant variable is '{most_significant_variable}' with a coefficient of {feature_coefficients[most_significant_variable]:.4f}")

# Sort features by their absolute coefficient values to see which have the largest impact
sorted_coefficients = feature_coefficients.abs().sort_values(ascending=False)

# Plot the absolute values of the coefficients
plt.figure(figsize=(12, 8))
sorted_coefficients.plot(kind='barh', color='skyblue')
plt.xlabel('Absolute Coefficient Value')
plt.title('Feature Importance in Logistic Regression Model')
plt.show()

# Evaluate the model's performance
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score

def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]

    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    print(f"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")

# Evaluate the logistic regression model
evaluate_model(log_reg, X_test, y_test)

"""Evaluate Each Model"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import seaborn as sns



data_cleaned = data.drop(columns=['DoctorInCharge'])

# Encode categorical variables (like Gender, Ethnicity) into dummy/indicator variables
categorical_columns = ['Gender', 'Ethnicity']
data_encoded = pd.get_dummies(data_cleaned, columns=categorical_columns, drop_first=True)

# Define the features (X) and the target variable (y)
X = data_encoded.drop(columns=['Diagnosis', 'PatientID'])
y = data_encoded['Diagnosis'].astype('int')

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Function to evaluate the model
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    print(confusion_matrix(y_test, y_pred))
    print(classification_report(y_test, y_pred))
    print("Accuracy:", accuracy_score(y_test, y_pred))
    if y_pred_proba is not None:
        print("ROC-AUC:", roc_auc_score(y_test, y_pred_proba))

# Logistic Regression
print("Logistic Regression:")
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)
evaluate_model(log_reg, X_test, y_test)

# Random Forest
print("\nRandom Forest:")
rf = RandomForestClassifier(random_state=42)
rf_param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10],
    'min_samples_split': [2, 5]
}
rf_grid_search = GridSearchCV(estimator=rf, param_grid=rf_param_grid, cv=5, n_jobs=-1, scoring='accuracy')
rf_grid_search.fit(X_train, y_train)
best_rf = rf_grid_search.best_estimator_
evaluate_model(best_rf, X_test, y_test)

# Feature Importance for Random Forest
importances = best_rf.feature_importances_
indices = np.argsort(importances)[::-1]
features = X.columns

plt.figure(figsize=(10, 6))
plt.title("Feature Importances - Random Forest")
plt.bar(range(X.shape[1]), importances[indices], align="center")
plt.xticks(range(X.shape[1]), [features[i] for i in indices], rotation=90)
plt.tight_layout()
plt.show()

# Gradient Boosting
print("\nGradient Boosting:")
gbm = GradientBoostingClassifier(random_state=42)
gbm_param_grid = {
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.1],
    'max_depth': [3, 4]
}
gbm_grid_search = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid, cv=5, n_jobs=-1, scoring='accuracy')
gbm_grid_search.fit(X_train, y_train)
best_gbm = gbm_grid_search.best_estimator_
evaluate_model(best_gbm, X_test, y_test)

# Feature Importance for Gradient Boosting
importances_gbm = best_gbm.feature_importances_
indices_gbm = np.argsort(importances_gbm)[::-1]

plt.figure(figsize=(10, 6))
plt.title("Feature Importances - Gradient Boosting")
plt.bar(range(X.shape[1]), importances_gbm[indices_gbm], align="center")
plt.xticks(range(X.shape[1]), [features[i] for i in indices_gbm], rotation=90)
plt.tight_layout()
plt.show()

# Support Vector Machine
print("\nSupport Vector Machine:")
svm = SVC(probability=True, random_state=42)
svm_param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}
svm_grid_search = GridSearchCV(estimator=svm, param_grid=svm_param_grid, cv=5, n_jobs=-1, scoring='accuracy')
svm_grid_search.fit(X_train, y_train)
best_svm = svm_grid_search.best_estimator_
evaluate_model(best_svm, X_test, y_test)

"""Logistic,Random Forest.Gradient Boosting,SVM,Neural Network"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import seaborn as sns
from keras.models import Sequential
from keras.layers import Dense


data_cleaned = data.drop(columns=['DoctorInCharge'])

# Encode categorical variables
categorical_columns = ['Gender', 'Ethnicity']
data_encoded = pd.get_dummies(data_cleaned, columns=categorical_columns, drop_first=True)

# Define the features (X) and the target variable (y)
X = data_encoded.drop(columns=['Diagnosis', 'PatientID'])
y = data_encoded['Diagnosis'].astype('int')

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Function to evaluate the model
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    print(confusion_matrix(y_test, y_pred))
    print(classification_report(y_test, y_pred))
    print("Accuracy:", accuracy_score(y_test, y_pred))
    if y_pred_proba is not None:
        print("ROC-AUC:", roc_auc_score(y_test, y_pred_proba))

# Logistic Regression
print("Logistic Regression:")
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)
evaluate_model(log_reg, X_test, y_test)

# Random Forest
print("\nRandom Forest:")
rf = RandomForestClassifier(random_state=42)
rf_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}
rf_grid_search = GridSearchCV(estimator=rf, param_grid=rf_param_grid, cv=5, n_jobs=-1, scoring='accuracy')
rf_grid_search.fit(X_train, y_train)
best_rf = rf_grid_search.best_estimator_
evaluate_model(best_rf, X_test, y_test)

# Feature Importance for Random Forest
importances = best_rf.feature_importances_
indices = np.argsort(importances)[::-1]
features = X.columns

plt.figure(figsize=(10, 6))
plt.title("Feature Importances - Random Forest")
plt.bar(range(X.shape[1]), importances[indices], align="center")
plt.xticks(range(X.shape[1]), [features[i] for i in indices], rotation=90)
plt.tight_layout()
plt.show()

# Gradient Boosting
print("\nGradient Boosting:")
gbm = GradientBoostingClassifier(random_state=42)
gbm_param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5]
}
gbm_grid_search = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid, cv=5, n_jobs=-1, scoring='accuracy')
gbm_grid_search.fit(X_train, y_train)
best_gbm = gbm_grid_search.best_estimator_
evaluate_model(best_gbm, X_test, y_test)

# Feature Importance for Gradient Boosting
importances_gbm = best_gbm.feature_importances_
indices_gbm = np.argsort(importances_gbm)[::-1]

plt.figure(figsize=(10, 6))
plt.title("Feature Importances - Gradient Boosting")
plt.bar(range(X.shape[1]), importances_gbm[indices_gbm], align="center")
plt.xticks(range(X.shape[1]), [features[i] for i in indices_gbm], rotation=90)
plt.tight_layout()
plt.show()

# Support Vector Machine
print("\nSupport Vector Machine:")
svm = SVC(probability=True, random_state=42)
svm_param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}
svm_grid_search = GridSearchCV(estimator=svm, param_grid=svm_param_grid, cv=5, n_jobs=-1, scoring='accuracy')
svm_grid_search.fit(X_train, y_train)
best_svm = svm_grid_search.best_estimator_
evaluate_model(best_svm, X_test, y_test)

# Neural Network
model = Sequential()
model.add(Dense(12, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=150, batch_size=10, validation_split=0.1)

# Predict class labels
y_pred_proba_nn = model.predict(X_test)
y_pred_nn = (y_pred_proba_nn > 0.5).astype("int32")

# Evaluate the neural network model
def evaluate_nn_model(y_true, y_pred, y_pred_proba):
    print(confusion_matrix(y_true, y_pred))
    print(classification_report(y_true, y_pred))
    print("Accuracy:", accuracy_score(y_true, y_pred))
    print("ROC-AUC:", roc_auc_score(y_true, y_pred_proba))

print("\nNeural Network:")
evaluate_nn_model(y_test, y_pred_nn, y_pred_proba_nn)

# Comparison of models
models = {
    'Logistic Regression': log_reg,
    'Random Forest': best_rf,
    'Gradient Boosting': best_gbm,
    'SVM': best_svm,
    'Neural Network': model
}

for model_name, model in models.items():
    if model_name == 'Neural Network':
        roc_auc = roc_auc_score(y_test, y_pred_proba_nn)
    else:
        roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test))
    print(f"\n{model_name} ROC-AUC: {roc_auc}")

"""Analysis and Results

Logistic Regression: ROC-AUC: ~0.92

Random Forest: ROC-AUC: ~0.96

Gradient Boosting: ROC-AUC: ~0.96

SVM: ROC-AUC: ~0.93

Neural Network: ROC-AUC: ~0.90


Logistic Regression: Achieved an accuracy of ~86% and an ROC-AUC score of ~0.92. This indicates good performance but room for improvement.

This score indicates a high level of discrimination by the model between the positive class (diagnosed with diabetes) and the negative class (not diagnosed).

The results indicate that the Logistic Regression model performs quite well with an accuracy of approximately 86.17% and an ROC-AUC score of approximately 0.92.



Random Forest: Performed better with an accuracy of ~92% and ROC-AUC of ~0.96. It effectively handles non-linear relationships and interactions between features.

Gradient Boosting: Similar to Random Forest, with an accuracy of ~94% and ROC-AUC of ~0.96. It shows the strength of boosting in improving model performance.

SVM: Achieved an accuracy of ~86% and an ROC-AUC of ~0.93. While robust, it was outperformed by ensemble methods.

Neural Network: Had an accuracy of ~82% and ROC-AUC of ~0.90. It performed reasonably well, but the ensemble methods were superior.

Best Model

Both the Random Forest and Gradient Boosting models have the highest ROC-AUC scores ~96%



However, the choice between Random Forest and Gradient Boosting may also depend on other factors such as:

Interpretability: Random Forests are generally easier to interpret, especially with feature importance plots.

Training Time: Gradient Boosting can be more computationally intensive and slower to train compared to Random Forests.

Considering these factors, Gradient Boosting is marginally better due to its slightly higher accuracy and ROC-AUC score. However, if interpretability and faster training time are more critical, Random Forest is also an excellent choice.

Final Recommendation
Gradient Boosting is the best model based on its higher accuracy and ROC-AUC score, making it the top choice for predicting the diagnosis in this dataset.

ROC-AUC is a crucial metric for evaluating the performance of classification models, particularly in scenarios with imbalanced data or when it's essential to consider various classification thresholds.

ROC curve
"""

!pip install tensorflow
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

# Remove the 'DoctorInCharge' column
data_cleaned = data.drop(columns=['DoctorInCharge'])

# Encode categorical variables (like Gender, Ethnicity) into dummy/indicator variables
categorical_columns = ['Gender', 'Ethnicity']
data_encoded = pd.get_dummies(data_cleaned, columns=categorical_columns, drop_first=True)

# Define the features (X) and the target variable (y)
X = data_encoded.drop(columns=['Diagnosis', 'PatientID'])
y = data_encoded['Diagnosis'].astype('int')

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the numerical features (scale them to have mean 0 and variance 1)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Logistic Regression
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)

# Random Forest
rf = RandomForestClassifier(random_state=42)
rf_param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10],
    'min_samples_split': [2, 5]
}
rf_grid_search = GridSearchCV(estimator=rf, param_grid=rf_param_grid, cv=3, n_jobs=-1, scoring='accuracy')
rf_grid_search.fit(X_train, y_train)
best_rf = rf_grid_search.best_estimator_

# Gradient Boosting
gbm = GradientBoostingClassifier(random_state=42)
gbm_param_grid = {
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.1],
    'max_depth': [3, 4]
}
gbm_grid_search = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid, cv=3, n_jobs=-1, scoring='accuracy')
gbm_grid_search.fit(X_train, y_train)
best_gbm = gbm_grid_search.best_estimator_

# Support Vector Machine
svm = SVC(probability=True, random_state=42)
svm_param_grid = {
    'C': [0.1, 1],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}
svm_grid_search = GridSearchCV(estimator=svm, param_grid=svm_param_grid, cv=3, n_jobs=-1, scoring='accuracy')
svm_grid_search.fit(X_train, y_train)
best_svm = svm_grid_search.best_estimator_

# Neural Network
def create_nn_model(optimizer='adam'):
    model = Sequential()
    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Fit the neural network directly without using GridSearchCV
nn_model = create_nn_model()
history = nn_model.fit(X_train, y_train, epochs=20, batch_size=10, validation_split=0.2, verbose=1)

# Evaluate the model
nn_score = nn_model.evaluate(X_test, y_test, verbose=0)

# Predict probabilities
y_pred_proba_nn = nn_model.predict(X_test).ravel()

# Function to plot ROC curve
def plot_roc_curve(model, X_test, y_test, model_name, y_pred_proba=None):
    if y_pred_proba is None:
        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else model.decision_function(X_test)
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    roc_auc = auc(fpr, tpr)

    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})')

plt.figure(figsize=(10, 8))

# Plot ROC curve for Logistic Regression
plot_roc_curve(log_reg, X_test, y_test, 'Logistic Regression')

# Plot ROC curve for Random Forest
plot_roc_curve(best_rf, X_test, y_test, 'Random Forest')

# Plot ROC curve for Gradient Boosting
plot_roc_curve(best_gbm, X_test, y_test, 'Gradient Boosting')

# Plot ROC curve for SVM
plot_roc_curve(best_svm, X_test, y_test, 'SVM')

# Plot ROC curve for Neural Network
plot_roc_curve(nn_model, X_test, y_test, 'Neural Network', y_pred_proba=y_pred_proba_nn)

plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid()
plt.show()

"""Gradient Boosting has the highest AUC (0.96), making it the best performer among the models.

Random Forest also performs very well with an AUC of 0.95.

Logistic Regression and SVM have slightly lower but still strong performances with AUC values of 0.92. Overall, all the models perform quite well, but Gradient Boosting and Random Forest outperform Logistic Regression and SVM in this particular classification task.

K fold cross validation
"""

import pandas as pd
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import make_scorer, roc_auc_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline


data_cleaned = data.drop(columns=['DoctorInCharge'])


categorical_columns = ['Gender', 'Ethnicity']
data_encoded = pd.get_dummies(data_cleaned, columns=categorical_columns, drop_first=True)

# Define the features (X) and the target variable (y)
X = data_encoded.drop(columns=['Diagnosis', 'PatientID'])
y = data_encoded['Diagnosis'].astype('int')

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)



data_cleaned = df.drop(columns=['DoctorInCharge'])
categorical_columns = ['Gender', 'Ethnicity']
data_encoded = pd.get_dummies(data_cleaned, columns=categorical_columns, drop_first=True)

# Define the features (X) and the target variable (y)
X = data_encoded.drop(columns=['Diagnosis', 'PatientID'])
y = data_encoded['Diagnosis'].astype('int')

# Define the cross-validation strategy
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Define models
logistic_model = LogisticRegression(max_iter=1000)
rf_model = RandomForestClassifier(random_state=42)
gb_model = GradientBoostingClassifier(random_state=42)
svm_model = SVC(probability=True, kernel='linear', random_state=42)
nn_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)

# Define a function to perform cross-validation and print results
def cross_validate_model(model, X, y, cv, model_name):
    # Create a pipeline with standardization and the model
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('model', model)
    ])

    # Perform cross-validation
    scores = cross_val_score(pipeline, X, y, cv=cv, scoring=make_scorer(roc_auc_score))

    # Print results
    print(f"{model_name} - Mean ROC-AUC: {scores.mean():.4f}, Std Dev: {scores.std():.4f}")

# Perform cross-validation for each model
cross_validate_model(logistic_model, X, y, cv, "Logistic Regression")
cross_validate_model(rf_model, X, y, cv, "Random Forest")
cross_validate_model(gb_model, X, y, cv, "Gradient Boosting")
cross_validate_model(svm_model, X, y, cv, "Support Vector Machine (SVM)")
cross_validate_model(nn_model, X, y, cv, "Neural Network")

"""Gradient Boosting stands out as the top-performing model with a mean ROC-AUC of 0.9180 and a low standard deviation of 0.0079, highlighting its superior accuracy and consistency.

Random Forest follows closely, achieving a mean ROC-AUC of 0.8932 with a slightly higher, yet still low, standard deviation of 0.0115, indicating it is also a strong and reliable model.

Logistic Regression and SVM deliver moderate performance, with mean ROC-AUCs of 0.8213 and 0.8176, respectively. However, their higher standard deviations (0.0224 and 0.0242) point to greater variability in their performance.

Neural Network is the least effective model, with the lowest mean ROC-AUC of 0.8054 and a moderate standard deviation of 0.0159, suggesting it is less reliable for this specific classification task.

Overall, Gradient Boosting is the most suitable model based on ROC-AUC performance, offering the best balance of accuracy and stability.

feature analysis
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
import matplotlib.pyplot as plt
import seaborn as sns




data_cleaned = data.drop(columns=['DoctorInCharge', 'PatientID'])

# Encode categorical variables
categorical_columns = ['Gender', 'Ethnicity']
data_encoded = pd.get_dummies(data_cleaned, columns=categorical_columns, drop_first=True)

# Define features (X) and target (y)
X = data_encoded.drop(columns=['Diagnosis'])
y = data_encoded['Diagnosis'].astype('int')

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

pip install scikit-optimize

"""Ensenble Methos

Ensemble learning involves combining multiple models to improve overall performance. Here, we'll look at three primary ensemble techniques: Stacking, Bagging, and Boosting.

1. Stacking:
Stacking combines different models (often of different types) and uses a meta-model to make the final prediction.
"""

from sklearn.ensemble import StackingClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score



# Define the base models
base_models = [
    ('lr', LogisticRegression(max_iter=1000, random_state=42)),
    ('rf', RandomForestClassifier(n_estimators=165, max_depth=11, max_features=0.76,
                                  min_samples_leaf=3, min_samples_split=5, random_state=42)),
    ('gb', GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42))
]

# Define the meta-model
meta_model = LogisticRegression(max_iter=1000, random_state=42)

# Create the StackingClassifier
stacking_clf = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)

# Train the stacking classifier
stacking_clf.fit(X_train, y_train)

# Evaluate the stacked model on the test set
y_pred = stacking_clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Stacked Model Test Set Accuracy: {accuracy:.4f}")

"""2. Bagging:
Bagging (Bootstrap Aggregating) involves training multiple instances of the same model on different subsets of the data and averaging their predictions. The most well-known example is the Random Forest.
"""

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

# Create a base model (e.g., Decision Tree)
base_model = DecisionTreeClassifier(random_state=42)

# Create the Bagging classifier
bagging_clf = BaggingClassifier(base_estimator=base_model, n_estimators=100, random_state=42)

# Train the Bagging classifier
bagging_clf.fit(X_train, y_train)

# Evaluate the Bagging model on the test set
y_pred = bagging_clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Bagging Model Test Set Accuracy: {accuracy:.4f}")

"""3. Boosting:
Boosting sequentially trains models, with each model trying to correct the errors made by the previous one. Gradient Boosting and AdaBoost are popular boosting methods.
"""

from sklearn.ensemble import GradientBoostingClassifier

# Create the Gradient Boosting classifier
gb_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42)

# Train the Gradient Boosting classifier
gb_clf.fit(X_train, y_train)

# Evaluate the Gradient Boosting model on the test set
y_pred = gb_clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Gradient Boosting Model Test Set Accuracy: {accuracy:.4f}")

"""Stacked Model Test Set Accuracy: 0.9521

Interpretation:
The stacking model outperformed both bagging and boosting in terms of accuracy, achieving 95.21%. This suggests that combining the strengths of different base models (e.g., Logistic Regression, Random Forest, Gradient Boosting) and using a meta-model (Logistic Regression) has led to a more robust and accurate model. The meta-model effectively learned how to best combine the predictions from the base models to optimize performance.

Bagging Model Test Set Accuracy: 0.9388

Interpretation:
The bagging model, which in this case was based on a decision tree classifier, achieved an accuracy of 93.88%. Bagging typically reduces the variance of the model, leading to more stable and reliable predictions. This result indicates that bagging is effective but was slightly outperformed by the stacking model in this scenario.

Gradient Boosting Model Test Set Accuracy: 0.9362

Interpretation:
The gradient boosting model achieved an accuracy of 93.62%. Boosting methods, like gradient boosting, are powerful because they sequentially focus on the errors of previous models, leading to improvements in accuracy. However, in this case, gradient boosting slightly underperformed compared to the bagging and stacking methods, which might suggest that the stacking methodâ€™s ability to combine diverse models outperformed the sequential improvements made by gradient boosting.

Stacking Provided the Best Results:

The stacking model yielded the highest accuracy, indicating that leveraging different model types and combining their outputs through a meta-model is particularly effective for your dataset.
Bagging and Boosting Also Performed Well:

Both bagging and gradient boosting performed very well, with accuracies above 93%. These methods are still robust and could be preferable in situations where simplicity or specific model characteristics (e.g., interpretability or resistance to overfitting) are prioritized.

Advanced Metrics and Cross-Validation
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score, auc
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split


# Train a RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Predict probabilities
y_scores = rf.predict_proba(X_test)[:, 1]

# Precision-Recall Curve
precision, recall, thresholds = precision_recall_curve(y_test, y_scores)
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, marker='.', label='RandomForest')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.show()

# ROC Curve and AUC
fpr, tpr, roc_thresholds = roc_curve(y_test, y_scores)
roc_auc = roc_auc_score(y_test, y_scores)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'RandomForest (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', label='Random Guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

"""Unsupervised Learning"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns




categorical_columns = ['Gender', 'Ethnicity']
data_encoded = pd.get_dummies(data_cleaned, columns=categorical_columns, drop_first=True)


X = data_encoded.drop(columns=['Diagnosis', 'PatientID'])


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

### K-Means Clustering ###

print("\nK-Means Clustering:")
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(X_scaled)
clusters = kmeans.predict(X_scaled)

#data_encoded['Cluster'] = clusters

# Plotting the clusters using PCA
pca = PCA(n_components=2)
principal_components = pca.fit_transform(X_scaled)
principal_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])
principal_df['Cluster'] = clusters

plt.figure(figsize=(10, 6))
plt.title("K-Means Clustering")
sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=principal_df, palette='viridis')
plt.show()

"""K-Means Clustering Plot Explanation
The K-Means clustering algorithm aims to partition the data into clusters based on feature similarity. In this case, we specified n_clusters=2, indicating that we want to divide the data into two distinct groups.

The scatter plot visualizes the results of the K-Means clustering using PCA (Principal Component Analysis) to reduce the data's dimensionality to two principal components (PC1 and PC2) for easy visualization. Each point represents a data sample, and the points are colored based on their assigned cluster (Cluster 0 or Cluster 1).

full script

Early Detection: Identifying individuals at risk of diabetes early can help in implementing preventive measures
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Training the model
model = LogisticRegression(random_state=42)
model.fit(X_train_scaled, y_train)

# Making predictions and calculating risk scores
y_pred = model.predict(X_test_scaled)
y_prob = model.predict_proba(X_test_scaled)[:, 1]

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)

# Printing evaluation metrics
print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1-Score: {f1:.2f}')
print(f'ROC-AUC: {roc_auc:.2f}')

# Plotting ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

# Determine threshold for high-risk classification
threshold = 0.5  # This can be adjusted based on desired sensitivity and specificity
high_risk = y_prob >= threshold

# Adding high-risk classification to the test set
X_test['HighRisk'] = high_risk
X_test['RiskScore'] = y_prob

# Display the high-risk patients
high_risk_patients = X_test[high_risk]
high_risk_patients

"""High-Risk Patients Statistics"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('diabetesnew.csv')

# Selecting features and target variable
X = data.drop(columns=['PatientID', 'Diagnosis', 'DoctorInCharge'])
y = data['Diagnosis']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardizing the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Training the model
model = LogisticRegression(random_state=42)
model.fit(X_train_scaled, y_train)

# Making predictions and calculating risk scores
y_pred = model.predict(X_test_scaled)
y_prob = model.predict_proba(X_test_scaled)[:, 1]

# Determine threshold for high-risk classification
threshold = 0.5  # This can be adjusted based on desired sensitivity and specificity
high_risk = y_prob >= threshold

# Adding high-risk classification to the test set
X_test = pd.DataFrame(X_test_scaled, columns=X.columns)
X_test['HighRisk'] = high_risk
X_test['RiskScore'] = y_prob
X_test['Diagnosis'] = y_test.values

# Display statistics of high-risk patients
high_risk_patients = X_test[high_risk]

# Calculating statistics for high-risk patients
high_risk_stats = high_risk_patients.describe()

print("High-Risk Patients Statistics:")
print(high_risk_stats)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)

# Printing evaluation metrics
print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1-Score: {f1:.2f}')
print(f'ROC-AUC: {roc_auc:.2f}')

# Plotting ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

"""Accuracy: 0.84
Precision: 0.80
Recall: 0.76
F1-Score: 0.78
ROC-AUC: 0.90
means                                                   Accuracy of 84%: Indicates the overall effectiveness of the model in correctly predicting both diabetes and non-diabetes cases.
Precision of 80%: Demonstrates that the model makes relatively few false positive predictions (predicting diabetes when the patient does not have it).
Recall of 76%: Shows that the model successfully identifies a significant portion of actual diabetes cases but may miss some.
F1-Score of 78%: Suggests a balanced performance in terms of precision and recall, making the model reliable for both identifying actual positives and avoiding false positives.
ROC-AUC of 90%: Highlights the model's strong discriminatory power, effectively differentiating between high-risk and low-risk patients.
These metrics collectively indicate that the model performs well, with high accuracy, precision, recall, F1-score, and ROC-AUC, making it a robust tool for identifying individuals at risk of diabetes for early intervention and preventive measures.                  
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Plotting histograms for continuous features
features = ['Age', 'BMI', 'QualityOfLifeScore','Gender', 'Smoking', 'AlcoholConsumption', 'PhysicalActivity']  # Add more features as needed

for feature in features:
    plt.figure(figsize=(10, 6))
    sns.histplot(high_risk_patients[feature], kde=True, bins=30)
    plt.title(f'Distribution of {feature} for High-Risk Patients')
    plt.xlabel(feature)
    plt.ylabel('Frequency')
    plt.show()

# Plotting bar charts for categorical features
categorical_features = ['Gender', 'Smoking', 'AlcoholConsumption', 'PhysicalActivity']  # Add more features as needed

for feature in categorical_features:
    plt.figure(figsize=(10, 6))
    sns.countplot(data=high_risk_patients, x=feature)
    plt.title(f'Count of {feature} for High-Risk Patients')
    plt.xlabel(feature)
    plt.ylabel('Count')
    plt.show()

# Plotting box plots for continuous features
for feature in features:
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=high_risk_patients, y=feature)
    plt.title(f'Box Plot of {feature} for High-Risk Patients')
    plt.ylabel(feature)
    plt.show()

"""Personalized Treatment for Diabetes
Personalized treatment involves tailoring medical treatment to the individual characteristics, needs, and preferences of each patient. By understanding which factors contribute to a diabetes diagnosis, healthcare providers can develop more effective, patient-specific treatment plans.

Factors Contributing to Diabetes Diagnosis
Age: Certain age groups may be more susceptible to diabetes.
Gender: There could be gender-specific factors influencing diabetes risk.
Ethnicity: Some ethnic groups have a higher predisposition to diabetes.
BMI: Higher BMI is often associated with a greater risk of diabetes.
Lifestyle Factors: Smoking, alcohol consumption, and physical activity levels can impact diabetes risk.
Socioeconomic Status: Access to healthcare, education, and economic stability can influence diabetes management.
Medical History: Previous health conditions, medication adherence, and frequency of medical checkups are crucial.
Implementing Personalized Treatment with Machine Learning
By analyzing these factors with a machine learning model, healthcare providers can:

Identify High-Risk Patients: Early identification of high-risk patients allows for timely intervention.
Customize Treatment Plans: Based on individual risk factors, treatment plans can be adjusted to address specific needs (e.g., weight management for patients with high BMI).
Monitor Progress: Continuous monitoring of at-risk individuals to adjust treatments as needed.
Provide Targeted Education: Educate patients on lifestyle changes that could reduce their risk.

Gender distribution
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set the style of seaborn plot
sns.set(style="whitegrid")

# Create a figure with two subplots
fig, ax = plt.subplots(1, 2, figsize=(14, 6))

# Plot 1: Gender distribution
sns.countplot(x='Gender', data=data, ax=ax[0], palette='pastel')
ax[0].set_title('Gender Distribution', fontsize=16)
ax[0].set_xlabel('Gender (0 = Male, 1 = Female)', fontsize=14)
ax[0].set_ylabel('Count', fontsize=14)

# Plot 2: Proportion of Diabetes Diagnosis by Gender
gender_risk = data.groupby('Gender')['Diagnosis'].mean()
sns.barplot(x=gender_risk.index, y=gender_risk.values, ax=ax[1], palette='pastel')
ax[1].set_title('Proportion of Diabetes Diagnosis by Gender', fontsize=16)
ax[1].set_xlabel('Gender (0 = Male, 1 = Female)', fontsize=14)
ax[1].set_ylabel('Proportion with Diabetes', fontsize=14)

# Show the plots
plt.tight_layout()
plt.show()

import shap
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

data = pd.read_csv('diabetesnew.csv')

# Define the features (X) and target (y)
X = data.drop(['Diagnosis'], axis=1)  # Replace 'Diagnosis' with your actual target column
y = data['Diagnosis']

# Identify and handle non-numeric columns if necessary
non_numeric_columns = X.select_dtypes(include=['object']).columns

# Option 1: Drop non-numeric columns
X = X.drop(non_numeric_columns, axis=1)

# Option 2: Alternatively, you could encode them using pd.get_dummies() if they are important:
# X = pd.get_dummies(X, columns=non_numeric_columns)

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

pip install pandas xgboost shap scikit-learn

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import shap


# Print the column names to identify the correct diagnosis column name
print("Column names in the dataset:", df.columns)

# Define the correct column names for dropping
X = df.drop(columns=['Diagnosis', 'DoctorInCharge'])
# Replace 'Diagnosis' with the correct column name
y = df['Diagnosis']  # Target: the diagnosis column

# Inspect the target variable
print("Unique values in target variable:", y.unique())

# Assuming 'Diagnosis' might have non-numeric values, let's convert it
# For example, if it has categorical values like 'Positive', 'Negative', we convert them to 1, 0 respectively
y = y.replace({'Positive': 1, 'Negative': 0})  # Adjust the mapping based on your actual data

# Check the unique values again to confirm
print("Unique values after conversion:", y.unique())

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train an XGBoost model
model = xgb.XGBClassifier(eval_metric='logloss')
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Calculate the accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Model accuracy: {accuracy}")

# Initialize SHAP Explainer
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Plot SHAP values for the first prediction
shap.initjs()
shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:])

# Plot SHAP summary plot
shap.summary_plot(shap_values, X_test)

"""Age Group Risk Analysis"""

import pandas as pd
import matplotlib.pyplot as plt


# Checking the distribution of diabetes (Diagnosis) across different age groups
age_groups = pd.cut(data['Age'], bins=[20, 30, 40, 50, 60, 70, 80, 90, 100])
diabetes_by_age_group = data.groupby(age_groups)['Diagnosis'].mean()

# Plotting the risk of diabetes by age group
plt.figure(figsize=(10, 6))
diabetes_by_age_group.plot(kind='bar', color='skyblue')
plt.title('Risk of Diabetes by Age Group')
plt.xlabel('Age Group')
plt.ylabel('Proportion of Diabetes Diagnosis')
plt.xticks(rotation=45)
plt.show()

# Display the age group with the highest risk
print("Age group with the highest risk of diabetes:", diabetes_by_age_group.idxmax())
print("Proportion of people diagnosed with diabetes in that age group:", diabetes_by_age_group.max())

"""Highest age group risk people belongs to (40,50) age group

Hyper tunning parameter
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score
import matplotlib.pyplot as plt




data = data.drop(columns=['DoctorInCharge'])


categorical_columns = ['Gender', 'Ethnicity']  # Adjust based on your actual categorical columns
data_encoded = pd.get_dummies(data, columns=categorical_columns, drop_first=True)

# Define the features (X) and the target variable (y)
X = data_encoded.drop(columns=['Diagnosis', 'PatientID'])  # Exclude non-feature columns
y = data_encoded['Diagnosis'].astype(int)  # Ensure the target variable is integer

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Function to evaluate the model
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    print(confusion_matrix(y_test, y_pred))
    print(classification_report(y_test, y_pred))
    print("Accuracy:", accuracy_score(y_test, y_pred))
    if y_pred_proba is not None:
        print("ROC-AUC:", roc_auc_score(y_test, y_pred_proba))

# Hyperparameter tuning for Logistic Regression
print("Logistic Regression:")
log_reg_param_grid = {
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear', 'saga'],
    'max_iter': [100, 200, 300]
}
log_reg = LogisticRegression()
log_reg_grid_search = GridSearchCV(log_reg, log_reg_param_grid, cv=5, scoring='roc_auc', n_jobs=-1)
log_reg_grid_search.fit(X_train, y_train)
best_log_reg = log_reg_grid_search.best_estimator_
evaluate_model(best_log_reg, X_test, y_test)

# Reduced parameter grid for Random Forest
print("\nRandom Forest:")
rf_param_grid = {
    'n_estimators': [100, 200],   # Reduced options
    'max_depth': [10, 20],        # Reduced options
    'min_samples_split': [5],     # Fixed at a single value
    'min_samples_leaf': [1],      # Fixed at a single value
    'bootstrap': [True]           # Fixed at a single value
}
rf = RandomForestClassifier(random_state=42)
rf_grid_search = GridSearchCV(rf, rf_param_grid, cv=3, scoring='roc_auc', n_jobs=-1)
rf_grid_search.fit(X_train, y_train)
best_rf = rf_grid_search.best_estimator_
evaluate_model(best_rf, X_test, y_test)

# Feature Importance for Random Forest
importances = best_rf.feature_importances_
indices = np.argsort(importances)[::-1]
features = X.columns

plt.figure(figsize=(10, 6))
plt.title("Feature Importances - Random Forest")
plt.bar(range(X.shape[1]), importances[indices], align="center")
plt.xticks(range(X.shape[1]), [features[i] for i in indices], rotation=90)
plt.tight_layout()
plt.show()

# Hyperparameter tuning for Gradient Boosting
print("\nGradient Boosting:")
gbm_param_grid = {
    'n_estimators': [100, 200],  # Reduced options
    'learning_rate': [0.01, 0.1],  # Reduced options
    'max_depth': [3, 4],  # Reduced options
    'min_samples_split': [2, 5],  # Reduced options
    'min_samples_leaf': [1, 2]  # Reduced options
}
gbm = GradientBoostingClassifier(random_state=42)
gbm_grid_search = GridSearchCV(gbm, gbm_param_grid, cv=3, scoring='roc_auc', n_jobs=-1)
gbm_grid_search.fit(X_train, y_train)
best_gbm = gbm_grid_search.best_estimator_
evaluate_model(best_gbm, X_test, y_test)

# Feature Importance for Gradient Boosting
importances_gbm = best_gbm.feature_importances_
indices_gbm = np.argsort(importances_gbm)[::-1]

plt.figure(figsize=(10, 6))
plt.title("Feature Importances - Gradient Boosting")
plt.bar(range(X.shape[1]), importances_gbm[indices_gbm], align="center")
plt.xticks(range(X.shape[1]), [features[i] for i in indices_gbm], rotation=90)
plt.tight_layout()
plt.show()

# Hyperparameter tuning for Support Vector Machine
print("\nSupport Vector Machine:")
svm_param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}
svm = SVC(probability=True, random_state=42)
svm_grid_search = GridSearchCV(svm, svm_param_grid, cv=3, scoring='roc_auc', n_jobs=-1)
svm_grid_search.fit(X_train, y_train)
best_svm = svm_grid_search.best_estimator_
evaluate_model(best_svm, X_test, y_test)

# Comparison of models
models = {
    'Logistic Regression': best_log_reg,
    'Random Forest': best_rf,
    'Gradient Boosting': best_gbm,
    'SVM': best_svm
}

for model_name, model in models.items():
    print(f"\n{model_name} ROC-AUC: {roc_auc_score(y_test, model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test))}")

"""Logistic Regression:

Accuracy: 83.8%
ROC-AUC: 0.906
Summary: Performs well with a good balance between precision and recall, but slightly lower accuracy and ROC-AUC compared to tree-based models.
Random Forest:

Accuracy: 90.9%
ROC-AUC: 0.953
Summary: Excellent performance, particularly in distinguishing between classes, with the highest ROC-AUC score.
Gradient Boosting:

Accuracy: 93.1%
ROC-AUC: 0.950
Summary: Highest overall accuracy with strong precision and recall, very close to Random Forest in ROC-AUC.
SVM:

Accuracy: 82.9%
ROC-AUC: 0.902
Summary: Decent performance, but lower accuracy and ROC-AUC compared to Random Forest and Gradient Boosting.
"""

